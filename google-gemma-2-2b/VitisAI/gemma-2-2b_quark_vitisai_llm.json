{
    "input_model": {
        "type": "HfModel",
        "model_path": "google/gemma-2-2b"
    },
    "passes": {
        "qq": {
            "type": "QuarkQuantization",
            "quant_scheme": "uint4_wo_128",
            "quant_algo": "awq",
            "dataset": "pileval_for_awq_benchmark",
            "data_type": "bfloat16",
            "num_calib_data": 128,
            "model_export": ["hf_format"],
            "exclude_layers": []
        },
        "mg": {
            "type": "VitisGenerateModelLLM",
            "optimize": "decode",
            "use_ep": true
        }
    },
    "log_severity_level": 1,
    "output_dir": "models/gemma-2-2b-vai",
    "cache_dir": "cache",
    "no_artifacts": true
}